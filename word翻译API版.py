"""
Wordæ–‡æ¡£æ™ºèƒ½ç¿»è¯‘å·¥å…·
åŠŸèƒ½ï¼šè‡ªåŠ¨ç¿»è¯‘ä¸­æ–‡Wordæ–‡æ¡£ä¸ºè‹±æ–‡ï¼Œä¿ç•™åŸæœ‰æ ¼å¼ï¼Œæ”¯æŒç¼“å­˜ã€é‡è¯•ã€å¤šçº¿ç¨‹ç­‰ç‰¹æ€§
"""

# æ ‡å‡†åº“å¯¼å…¥
import hashlib    # ç”¨äºç”ŸæˆMD5ç­¾å
import random     # ç”Ÿæˆéšæœºæ•°
import requests   # å‘é€HTTPè¯·æ±‚
import os         # æ–‡ä»¶è·¯å¾„æ“ä½œ
import json       # ç¼“å­˜æ–‡ä»¶è¯»å†™
import threading  # çº¿ç¨‹é”
import argparse   # å‘½ä»¤è¡Œå‚æ•°è§£æ
import time       # æ—¶é—´æ§åˆ¶
import re         # æ­£åˆ™è¡¨è¾¾å¼å¤„ç†
from concurrent.futures import ThreadPoolExecutor, as_completed  # çº¿ç¨‹æ± ç®¡ç†
from docx import Document  # Wordæ–‡æ¡£æ“ä½œ
from docx.shared import RGBColor  # é¢œè‰²è®¾ç½®
from docx.oxml import OxmlElement  # æ“ä½œWord XMLå…ƒç´ 
from docx.text.paragraph import Paragraph  # æ®µè½æ“ä½œ
from docx.oxml.ns import qn  # XMLå‘½åç©ºé—´å¤„ç†
from tqdm import tqdm  # è¿›åº¦æ¡æ˜¾ç¤º

# ========== å…¨å±€é…ç½® ==========
appid = '20241223002235907'      # ç™¾åº¦ç¿»è¯‘APIåº”ç”¨IDï¼ˆéœ€è‡ªè¡Œç”³è¯·ï¼‰
secretKey = 'FaoqgT8hOE5JLFoySt9S'  # ç™¾åº¦ç¿»è¯‘APIå¯†é’¥
url = 'https://fanyi-api.baidu.com/api/trans/vip/translate'  # APIç«¯ç‚¹
MAX_RETRIES = 10    # æœ€å¤§é‡è¯•æ¬¡æ•°
MAX_WORKERS = 4     # æœ€å¤§çº¿ç¨‹æ•°
CACHE_FILE = 'translation_cache.json'  # ç¼“å­˜æ–‡ä»¶è·¯å¾„

# ========== åˆå§‹åŒ–ç¼“å­˜ä¸é” ==========
# åŠ è½½ç¿»è¯‘ç¼“å­˜ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
if os.path.exists(CACHE_FILE):
    with open(CACHE_FILE, 'r', encoding='utf-8') as f:
        translation_cache = json.load(f)
else:
    translation_cache = {}

# åˆ›å»ºçº¿ç¨‹é”
cache_lock = threading.Lock()   # ç¼“å­˜è®¿é—®é”
print_lock = threading.Lock()   # æ§åˆ¶å°è¾“å‡ºé”

def highlight_paragraph(paragraph):
    """ä¸ºæ®µè½æ·»åŠ é»„è‰²é«˜äº®æ•ˆæœï¼ˆç”¨äºæ ‡è®°ç¿»è¯‘å¤±è´¥çš„æ®µè½ï¼‰"""
    for run in paragraph.runs:
        # åˆ›å»ºXMLé«˜äº®å…ƒç´ 
        rPr = run._element.get_or_add_rPr()
        highlight = OxmlElement('w:highlight')
        highlight.set(qn('w:val'), 'yellow')  # è®¾ç½®é«˜äº®é¢œè‰²ä¸ºé»„è‰²
        rPr.append(highlight)

def save_cache():
    """å°†ç¼“å­˜ä¿å­˜åˆ°æ–‡ä»¶ï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰"""
    with cache_lock:
        with open(CACHE_FILE, 'w', encoding='utf-8') as f:
            json.dump(translation_cache, f, ensure_ascii=False, indent=2)

def baidu_translate(text, attempt):
    """
    è°ƒç”¨ç™¾åº¦ç¿»è¯‘APIè¿›è¡Œå•æ¬¡ç¿»è¯‘å°è¯•
    :param text: å¾…ç¿»è¯‘æ–‡æœ¬
    :param attempt: å½“å‰å°è¯•æ¬¡æ•°ï¼ˆç”¨äºé”™è¯¯æç¤ºï¼‰
    :return: ç¿»è¯‘ç»“æœæˆ–None
    """
    # ç”Ÿæˆéšæœºç›å€¼
    salt = random.randint(32768, 65536)
    # æ„é€ ç­¾å
    sign_str = appid + text + str(salt) + secretKey
    sign = hashlib.md5(sign_str.encode()).hexdigest()
    
    # è¯·æ±‚å‚æ•°
    params = {
        'q': text,
        'from': 'zh',
        'to': 'en',
        'appid': appid,
        'salt': salt,
        'sign': sign
    }
    
    try:
        # å‘é€è¯·æ±‚
        r = requests.get(url, params=params, timeout=10)
        r.raise_for_status()  # æ£€æŸ¥HTTPçŠ¶æ€ç 
        
        result = r.json()
        # éªŒè¯å“åº”ç»“æ„
        if 'trans_result' not in result:
            raise ValueError(f"Invalid response: {result}")
            
        return result['trans_result'][0]['dst']
    except Exception as e:
        # è¾“å‡ºé”™è¯¯ä¿¡æ¯ï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰
        with print_lock:
            tqdm.write(f"âš ï¸ å°è¯• {attempt} å¤±è´¥: {text[:20]}... ({str(e)})")
        return None

def baidu_translate_cached(text):
    """
    å¸¦ç¼“å­˜å’Œé‡è¯•æœºåˆ¶çš„ç¿»è¯‘å‡½æ•°
    :param text: å¾…ç¿»è¯‘æ–‡æœ¬
    :return: ç¿»è¯‘ç»“æœæˆ–None
    """
    # æ£€æŸ¥ç¼“å­˜ï¼ˆå¦‚æœæœªç¦ç”¨ç¼“å­˜ï¼‰
    if not args.nocache:
        with cache_lock:
            if text in translation_cache:
                return translation_cache[text]

    # ç‰¹æ®Šå¤„ç†ï¼šç›´æ¥æ˜ å°„"å›å½’æµ‹è¯•"
    if 'å›å½’æµ‹è¯•' in text:
        translation_cache[text] = 'Regression testing'
        return 'Regression testing'

    # å¤„ç†å¸¦ç¼–å·çš„æ–‡æœ¬ï¼ˆå¦‚"1. æµ‹è¯•å†…å®¹"ï¼‰
    # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…ç¼–å·ç»“æ„
    match = re.match(r'^(\d+(?:\.\d+)*)([\s:ï¼šï¼Œ,]+)(.+)$', text)
    if match:
        prefix = match.group(1)    # ç¼–å·éƒ¨åˆ†ï¼ˆå¦‚"1.2.3"ï¼‰
        separator = match.group(2) # åˆ†éš”ç¬¦ï¼ˆå¦‚":"æˆ–ç©ºæ ¼ï¼‰
        rest = match.group(3)      # å®é™…éœ€è¦ç¿»è¯‘çš„å†…å®¹

        # ä»…ç¿»è¯‘å†…å®¹éƒ¨åˆ†
        translated_rest = None
        for attempt in range(1, MAX_RETRIES + 1):
            translated_rest = baidu_translate(rest.strip(), attempt)
            if translated_rest:
                break
            # æŒ‡æ•°é€€é¿ç­–ç•¥ï¼š2^attemptç§’ + éšæœºå»¶è¿Ÿï¼ˆé¿å…è¯·æ±‚é£æš´ï¼‰
            time.sleep(2 ** min(attempt, 5) + random.random())

        if translated_rest:
            # ç»„åˆç¼–å·å’Œç¿»è¯‘ç»“æœ
            result = f"{prefix}{separator}{translated_rest}"
            with cache_lock:
                translation_cache[text] = result
            return result
        else:
            return None

    # å¸¸è§„ç¿»è¯‘æµç¨‹
    translated = None
    for attempt in range(1, MAX_RETRIES + 1):
        translated = baidu_translate(text, attempt)
        if translated:
            # é‡è¯•æˆåŠŸæç¤º
            if attempt > 1:
                with print_lock:
                    tqdm.write(f"âœ… é‡è¯•åæˆåŠŸ: {text[:30]} (ç¬¬ {attempt} æ¬¡)")
            # æ›´æ–°ç¼“å­˜
            with cache_lock:
                translation_cache[text] = translated
            return translated
        # æŒ‡æ•°é€€é¿ç­–ç•¥
        time.sleep(2 ** min(attempt, 5) + random.random())

    return None

def split_into_sentences(text):
    """
    å°†æ–‡æœ¬åˆ†å‰²æˆå¥å­
    :param text: åŸå§‹æ–‡æœ¬
    :return: å¥å­åˆ—è¡¨
    """
    # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼åˆ†å‰²å¥å­ï¼š
    # - åœ¨å¥å·ã€æ„Ÿå¹å·ã€é—®å·ã€åˆ†å·ååˆ†å‰²
    # - åœ¨æ¢è¡Œç¬¦å‰åˆ†å‰²
    # - åœ¨æ•°å­—ç¼–å·ï¼ˆå¦‚"1."ï¼‰å‰åˆ†å‰²
    return [s.strip() for s in re.split(r'(?<=[ã€‚ï¼ï¼Ÿï¼›])|(?=\n)|(?=\d+\.)', text) if s.strip()]

def is_title(paragraph):
    """
    åˆ¤æ–­æ®µè½æ˜¯å¦ä¸ºæ ‡é¢˜ï¼ˆæ ¹æ®å­—ä½“å¤§å°ï¼‰
    :param paragraph: Wordæ®µè½å¯¹è±¡
    :return: æ˜¯å¦æ˜¯æ ‡é¢˜
    """
    if paragraph.runs:
        font_size = paragraph.runs[0].font.size
        # åˆ¤æ–­å­—ä½“å¤§å°æ˜¯å¦æ¥è¿‘16ptï¼ˆæ ‡é¢˜å¸¸è§å¤§å°ï¼‰
        if font_size and abs(font_size.pt - 16) < 1:
            return True
    return False
    # if paragraph.runs:
    #     font_size = paragraph.runs[0].font.size
    #     # åˆ¤æ–­å­—ä½“å¤§å°æ˜¯å¦æ¥è¿‘16ptï¼ˆæ ‡é¢˜å¸¸è§å¤§å°ï¼‰
    #     if font_size == 'ä¸‰å·':
    #         return True
    # return False

def process_paragraph(paragraph, pbar, stats):
    """
    å¤„ç†å•ä¸ªæ®µè½ï¼ˆçº¿ç¨‹æ‰§è¡Œå•å…ƒï¼‰
    :param paragraph: Wordæ®µè½å¯¹è±¡
    :param pbar: è¿›åº¦æ¡å¯¹è±¡
    :param stats: ç»Ÿè®¡ä¿¡æ¯å­—å…¸
    """
    text = paragraph.text.strip()
    if not text:
        pbar.update(1)
        return

    # åˆ†å‰²æ–‡æœ¬ä¸ºå¤šä¸ªå¥å­
    parts = split_into_sentences(text)
    combined_result = []  # ç»„åˆåçš„ç¿»è¯‘ç»“æœ
    success = True        # æ˜¯å¦å…¨éƒ¨ç¿»è¯‘æˆåŠŸ

    # é€å¥ç¿»è¯‘
    for part in parts:
        translated = baidu_translate_cached(part)
        if translated:
            combined_result.append(translated)
        else:
            combined_result.append(f"[ç¿»è¯‘å¤±è´¥: {part}]")
            success = False

    # å¤„ç†æ ‡é¢˜çš„ç‰¹æ®Šæƒ…å†µ
    if is_title(paragraph):
        # åœ¨åŸæ–‡åè¿½åŠ ç¿»è¯‘ï¼ˆä¿ç•™æ ‡é¢˜æ ¼å¼ï¼‰
        paragraph.add_run(' ' + ' '.join(combined_result))
    else:
        # åˆ›å»ºæ–°æ®µè½æ’å…¥ç¿»è¯‘ç»“æœ
        new_para = insert_paragraph_after(paragraph)
        run = new_para.add_run(' '.join(combined_result))
        
        # å¤åˆ¶åŸæ®µè½æ ·å¼
        if paragraph.runs:
            copy_run_style(paragraph.runs[0], run)
        
        # æ ‡è®°éƒ¨åˆ†å¤±è´¥çš„æ®µè½
        if not success:
            run.font.color.rgb = RGBColor(255, 255, 0)  # è®¾ç½®æ–‡å­—ä¸ºé»„è‰²
            highlight_paragraph(new_para)  # æ·»åŠ èƒŒæ™¯é«˜äº®

    # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
    if success:
        stats['success'] += 1
    else:
        stats['failed'] += 1

    save_cache()
    pbar.update(1)

def insert_paragraph_after(paragraph):
    """åœ¨å½“å‰æ®µè½ä¹‹åæ’å…¥æ–°æ®µè½"""
    new_p = OxmlElement("w:p")
    paragraph._p.addnext(new_p)
    return Paragraph(new_p, paragraph._parent)

def copy_run_style(source_run, target_run):
    """å¤åˆ¶æ–‡å­—æ ·å¼åˆ°ç›®æ ‡Run"""
    try:
        # å¤åˆ¶åŸºç¡€æ ·å¼
        target_run.bold = source_run.bold
        target_run.italic = source_run.italic
        target_run.underline = source_run.underline
        target_run.font.name = source_run.font.name
        target_run.font.size = source_run.font.size
        
        # å¤„ç†ä¸­æ–‡å­—ä½“è®¾ç½®
        if target_run._element.rPr is None:
            target_run._element.get_or_add_rPr()
        if source_run.font.name:
            target_run._element.rPr.rFonts.set(qn('w:eastAsia'), source_run.font.name)
    except Exception as e:
        pass

def translate_paragraphs(paragraphs):
    """æ‰§è¡Œå¤šçº¿ç¨‹ç¿»è¯‘"""
    stats = {'success': 0, 'failed': 0}
    # è®¡ç®—å®é™…éœ€è¦ç¿»è¯‘çš„æ®µè½æ•°ï¼ˆè¿‡æ»¤ç©ºæ®µè½ï¼‰
    total = len([p for p in paragraphs if p.text.strip()])

    with tqdm(total=total, desc="ç¿»è¯‘è¿›åº¦", ncols=80) as pbar:
        with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
            futures = []
            # æäº¤ç¿»è¯‘ä»»åŠ¡
            for p in paragraphs:
                if p.text.strip():
                    future = executor.submit(process_paragraph, p, pbar, stats)
                    futures.append(future)

            # ç­‰å¾…ä»»åŠ¡å®Œæˆ
            for future in as_completed(futures):
                try:
                    future.result()
                except Exception as e:
                    with print_lock:
                        tqdm.write(f"âš ï¸ çº¿ç¨‹é”™è¯¯: {str(e)}")

    return stats

def run_translate(file_path):
    """ä¸»æ‰§è¡Œå‡½æ•°"""
    global translation_cache
    
    # å¤„ç†ç¼“å­˜ç›¸å…³å‚æ•°
    if args.resetcache and os.path.exists(CACHE_FILE):
        os.remove(CACHE_FILE)
        translation_cache = {}
        print("ğŸ—‘ å·²æ¸…ç©ºç¼“å­˜æ–‡ä»¶")
    elif not os.path.exists(CACHE_FILE):
        translation_cache = {}
    elif not args.nocache:
        with open(CACHE_FILE, 'r', encoding='utf-8') as f:
            translation_cache = json.load(f)

    # åŠ è½½Wordæ–‡æ¡£
    print(f"ğŸ“„ æ­£åœ¨å¤„ç†æ–‡æ¡£ï¼š{file_path}")
    doc = Document(file_path)

    # æ”¶é›†æ‰€æœ‰éœ€è¦ç¿»è¯‘çš„æ®µè½
    all_paragraphs = []
    all_paragraphs.extend(doc.paragraphs)  # æ·»åŠ æ­£æ–‡æ®µè½
    
    # æ·»åŠ è¡¨æ ¼ä¸­çš„æ®µè½
    for table in doc.tables:
        for row in table.rows:
            for cell in row.cells:
                all_paragraphs.extend(cell.paragraphs)

    print(f"ğŸ“ å‘ç°å¯ç¿»è¯‘æ®µè½: {len(all_paragraphs)} ä¸ª")
    
    # æ‰§è¡Œç¿»è¯‘
    stats = translate_paragraphs(all_paragraphs)

    # ä¿å­˜ç»“æœæ–‡æ¡£
    base_name, ext = os.path.splitext(file_path)
    new_name = base_name + "_translate" + ext
    doc.save(new_name)
    print(f"\nâœ… ç¿»è¯‘å®Œæˆ - æˆåŠŸ: {stats['success']}, å¤±è´¥: {stats['failed']}")
    print(f"ğŸ“ æ–‡ä»¶å·²ä¿å­˜ä¸ºï¼š{new_name}")

if __name__ == "__main__":
    # å‘½ä»¤è¡Œå‚æ•°è§£æ
    parser = argparse.ArgumentParser(description="Wordæ–‡æ¡£æ™ºèƒ½ç¿»è¯‘å·¥å…·")
    parser.add_argument('-w', '--wordfile', required=True, help='Wordæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--nocache', action='store_true', help='å¿½ç•¥ç¼“å­˜å¹¶å¼ºåˆ¶é‡æ–°ç¿»è¯‘')
    parser.add_argument('--resetcache', action='store_true', help='æ¸…ç©ºç¼“å­˜åå†ç¿»è¯‘')
    args = parser.parse_args()
    
    # å¯åŠ¨ç¿»è¯‘æµç¨‹
    run_translate(args.wordfile)